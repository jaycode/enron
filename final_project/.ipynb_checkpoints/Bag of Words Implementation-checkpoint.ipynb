{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Implementation For Enron Emails\n",
    "We want to know the quality when using ONLY bag of words as features. Later in \"Identifying Fraud from Enron Email\" we will try combining this with our previous features (which unfortunately requires us to vectorize again but on full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### data_dict preparation. In this section we do all the adjustments we\n",
    "### made to dataset in \"Identifying Fraud from Enron Email\" document.\n",
    "\n",
    "### Warning! running this script will take a really long time, not to mention that you will need a copy of \n",
    "### Enron's email addresses from Udacity's Into to Machine Learning course.\n",
    "### Download this file instead (400+Mb) https://www.dropbox.com/s/6o79pqvxrp0xylv/enron_emails_all_words.pkl?dl=0\n",
    "\n",
    "# Change this to where you want all words pickle document to be kept.\n",
    "all_words_path = 'D:\\\\Projects\\\\data_science\\\\nanodegree_data_analyst\\\\intro_to_ml\\\\enron_emails_all_words.pkl'\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = cPickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "data_dict_updated = data_dict\n",
    "data_dict_updated['BELFER ROBERT']['deferred_income'] = -102500\n",
    "data_dict_updated['BELFER ROBERT']['deferral_payments'] = 'NaN'\n",
    "data_dict_updated['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict_updated['BELFER ROBERT']['director_fees'] = 102500\n",
    "data_dict_updated['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict_updated['BELFER ROBERT']['exercised_stock_options'] = 'NaN'\n",
    "data_dict_updated['BELFER ROBERT']['restricted_stock'] = 44093\n",
    "data_dict_updated['BELFER ROBERT']['restricted_stock_deferred'] = -44093\n",
    "data_dict_updated['BELFER ROBERT']['total_stock_value'] = 'NaN'\n",
    "\n",
    "data_dict_updated['BHATNAGAR SANJAY']['other'] = 'NaN'\n",
    "data_dict_updated['BHATNAGAR SANJAY']['expenses'] = 137864\n",
    "data_dict_updated['BHATNAGAR SANJAY']['director_fees'] = 'NaN'\n",
    "data_dict_updated['BHATNAGAR SANJAY']['total_payments'] = 137864\n",
    "data_dict_updated['BHATNAGAR SANJAY']['exercised_stock_options'] = 15456290\n",
    "data_dict_updated['BHATNAGAR SANJAY']['restricted_stock'] = 2604490\n",
    "data_dict_updated['BHATNAGAR SANJAY']['restricted_stock_deferred'] = -2604490\n",
    "data_dict_updated['BHATNAGAR SANJAY']['total_stock_value'] = 15456290\n",
    "\n",
    "data_dict_updated_zero = data_dict_updated\n",
    "for key, value in data_dict_updated_zero.items():\n",
    "    for item_key, item_value in value.items():\n",
    "        if value[item_key] == 'NaN':\n",
    "            data_dict_updated_zero[key][item_key] = 0\n",
    "            \n",
    "data_dict_fe = data_dict_updated_zero\n",
    "\n",
    "for name, item in data_dict_fe.items():\n",
    "    data_dict_fe[name]['from_this_person_to_poi_ratio'] = 0\n",
    "    data_dict_fe[name]['from_poi_to_this_person_ratio'] = 0\n",
    "    if item['from_messages'] > 0:\n",
    "        data_dict_fe[name]['from_this_person_to_poi_ratio'] = float(item['from_this_person_to_poi']) / float(item['from_messages'])\n",
    "    if item['to_messages'] > 0:\n",
    "        data_dict_fe[name]['from_poi_to_this_person_ratio'] = float(item['from_poi_to_this_person']) / float(item['to_messages'])\n",
    "\n",
    "features_list_fe = [\n",
    "    'poi',\n",
    "    'bonus',\n",
    "    'salary',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'from_poi_to_this_person_ratio',\n",
    "    'from_this_person_to_poi_ratio',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary',\n",
    "    'shared_receipt_with_poi'\n",
    "]\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "word_data = []\n",
    "key_data = []\n",
    "email_types = []\n",
    "poi_data = []\n",
    "# temp_counter = 0\n",
    "\n",
    "if (os.path.isfile(all_words_path)):\n",
    "    all_words = cPickle.load(open(all_words_path, \"r\"))\n",
    "    word_data, key_data, email_types, poi_data = zip(*all_words)\n",
    "else:\n",
    "    dirpath = 'emails_by_address/'\n",
    "    filenames = next(os.walk(dirpath))[2]\n",
    "    for key, value in data_dict_fe.items():\n",
    "        if value['email_address'] != 0:\n",
    "            paths = [\n",
    "                dirpath + 'from_' + value['email_address'] + '.txt',\n",
    "                dirpath + 'to_' + value['email_address'] + '.txt',\n",
    "            ]\n",
    "            for idx, path in enumerate(paths):\n",
    "                if os.path.isfile(path):\n",
    "                    print \"reading\", path, '...'\n",
    "                    email_list = open(path, 'r')\n",
    "                    for email_path in email_list:\n",
    "                        ### only look at first 100 emails when developing\n",
    "                        ### once everything is working, remove this line to run over full dataset\n",
    "                        # temp_counter += 1\n",
    "                        # if temp_counter < 100:\n",
    "                        email_path = '..' + email_path[19:-2]\n",
    "                        if os.path.isfile(email_path):\n",
    "                            email = open(email_path, 'r')\n",
    "                            text = parseOutText(email)\n",
    "                            word_data.append(text)\n",
    "                            key_data.append(key)\n",
    "                            poi_data.append(data_dict_fe[key]['poi'])\n",
    "                            if idx == 0:\n",
    "                                email_types.append('from')\n",
    "                            else:\n",
    "                                email_types.append('to')\n",
    "    all_words = zip(word_data, key_data, email_types, poi_data)\n",
    "    cPickle.dump(all_words, open(all_words_path, \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'skeannsf dear all further to the prc committe meet pleas find attach the first draft of the peer group map by function it was decid that each member would review the propos to move from 4 peer group to 3 for mid year the attach was complet some week ago by the hr lead in your busi unit and is base on a mckinsey studi request by jeff skill and steve kean under your review of the attach inform pleas focus on the final two page that show the strawman on how the function fell out under the propos peer group structur i have copi in the hr lead for your busi area they have been instrument in complet the map and will meet with you to provid ani specif inform you requir concern placement with your busi area i would point out that there doe appear to be some inconsist in function placement within the peer group for some busi unit it will be our primari focus at the next meet to resolv these issu your assist have been contact regard the time and date of our next meet under separ cover for our london colleagu we will aim for a morn meet david oxley',\n",
       " 'HAEDICKE MARK E',\n",
       " 'to',\n",
       " False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random = 13\n",
    "\n",
    "from sklearn import cross_validation\n",
    "words_train, words_test, labels_train, labels_test = cross_validation.train_test_split(word_data, poi_data, test_size=0.1, random_state=random)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=0.1,\n",
    "                             max_features=100, stop_words='english')\n",
    "import cProfile\n",
    "\n",
    "# Use cProfile if you need detailed time analysis\n",
    "# cProfile.run('features_train = vectorizer.fit_transform(words_train)')\n",
    "# cProfile.run('features_test  = vectorizer.transform(words_test)')\n",
    "features_train = vectorizer.fit_transform(words_train)\n",
    "features_test  = vectorizer.transform(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221303, 100)\n"
     ]
    }
   ],
   "source": [
    "print features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24590, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2001', u'addit', u'ani', u'ask', u'attach', u'avail', u'busi', u'california', u'cc', u'chang', u'comment', u'committe', u'compani', u'contact', u'continu', u'current', u'date', u'david', u'day', u'develop', u'direct', u'discuss', u'email', u'energi', u'enron', u'execut', u'follow', u'forward', u'friday', u'gas', u'group', u'help', u'includ', u'inform', u'issu', u'jeff', u'john', u'just', u'know', u'let', u'like', u'list', u'look', u'make', u'manag', u'mark', u'market', u'meet', u'messag']\n"
     ]
    }
   ],
   "source": [
    "fnames = vectorizer.get_feature_names()\n",
    "print fnames[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of top 5 word scores of METTS MARK's first message:\n",
      "comment - 0.40\n",
      "respons - 0.39\n",
      "review - 0.38\n",
      "john - 0.37\n",
      "includ - 0.34\n",
      "\n",
      "List of top 5 word scores of LAY KENNETH L's first message:\n",
      "work - 1.00\n",
      "2000 - 0.00\n",
      "2001 - 0.00\n",
      "addit - 0.00\n",
      "ani - 0.00\n"
     ]
    }
   ],
   "source": [
    "def top_5_word_scores(key):\n",
    "    idx = key_data.index(key)\n",
    "    # Not the most straightforward code but this is how to convert a row\n",
    "    # of sparse matrix into a list\n",
    "    row = features_train[idx].todense()[0].tolist()[0]\n",
    "\n",
    "    word_scores = zip(range(0, features_train.shape[1]), row)\n",
    "    sorted_scores = sorted(word_scores, key=lambda t: t[1] * -1)[:5]\n",
    "    #  [(129753, 0.3526242375705134), (117135, 0.34794669254575034), (170214, 0.3069845443039424), (117128, 0.22619103462072823), (91625, 0.2245133019602352)]\n",
    "    print(\"List of top 5 word scores of %s's first message:\" % key_data[idx])\n",
    "    for score in sorted_scores:\n",
    "        print(\"%s - %0.2f\" % (fnames[score[0]], score[1]))\n",
    "        \n",
    "top_5_word_scores('METTS MARK')\n",
    "print \"\"\n",
    "top_5_word_scores('LAY KENNETH L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=random)\n",
    "\n",
    "# cProfile.run('clf.fit(features_train, labels_train)')\n",
    "clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the quality metrics as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=13, splitter='best')\n",
      "\tAccuracy: 0.85665\tPrecision: 0.65509\tRecall: 0.25822\tF1: 0.37042\tF2: 0.29382\n",
      "Wall time: 55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(labels_test, pred, average='binary')\n",
    "f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print clf\n",
    "print PERF_FORMAT_STRING.format(accuracy, precision, recall, fscore, f2, display_precision = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
